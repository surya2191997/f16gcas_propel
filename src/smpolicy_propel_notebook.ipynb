{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smpropel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GiV4eoHV8-R"
      },
      "source": [
        "This work tries to develop and test a method for synthesizing state machines from neural policies for the F-16 GCAS task. On a high level it modifies [PROPEL](https://arxiv.org/abs/1907.05431) using [Jeevanaâ€™s method](https://jinala.github.io/assets/papers/iclr2020.pdf), to synthesize state machines, instead of a programmatic policy with a pre-specified DSL. \n",
        "\n",
        "Start by installing the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekW9vQmIW1x5"
      },
      "source": [
        "# !git clone https://github.com/openai/spinningup.git\n",
        "# !pip install -e /spinningup/.\n",
        "# !pip install csaf\n",
        "# !pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAk_Mx4Sachw"
      },
      "source": [
        "First we train a neural policy using ddpg, and save the pytorch model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQIxuPQLWB1f"
      },
      "source": [
        "from spinup.utils.run_utils import ExperimentGrid\n",
        "from spinup import ddpg_pytorch\n",
        "from spinup.algos.pytorch.ddpg.core import *\n",
        "from spinup.algos.pytorch.ddpg.ddpg import *\n",
        "import gym\n",
        "import argparse\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import csaf\n",
        "import csaf.config as cconf\n",
        "import csaf.system as csys\n",
        "\n",
        "\n",
        "# function for setting up environment\n",
        "def env_fn():\n",
        "    my_conf = cconf.SystemConfig.from_toml(\n",
        "        \"/home/sdwivedi/csaf_architecture/examples/f16/f16_simple_config.toml\")  \n",
        "    def ground_collision_condition(cname, outs):\n",
        "        \"\"\"ground collision premature termnation condition\"\"\"\n",
        "        return cname == \"plant\" and outs[\"states\"][11] <= 0.0\n",
        "    my_system = csys.System.from_config(my_conf)\n",
        "    my_env = csys.SystemEnv(\"autopilot\", my_system, terminating_conditions=ground_collision_condition)\n",
        "    return my_env \n",
        "\n",
        "def train_ddpg(args):\n",
        "    eg = ExperimentGrid(\"train_ddpg\")\n",
        "    eg.add('env_fn', env_fn)\n",
        "    eg.add('seed', [0])\n",
        "    eg.add('epochs', 50)    \n",
        "    eg.add('gamma', 0.97)     \n",
        "    eg.add('steps_per_epoch', 1000)\n",
        "    eg.add('save_freq', 1)\n",
        "    eg.add('max_ep_len', 2100)   \n",
        "    eg.add('update_after', 1000)\n",
        "    eg.add('ac_kwargs:hidden_sizes', [(256, 256)], 'hid')\n",
        "    eg.run(ddpg_pytorch)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--cpu\", type=int, default=1)\n",
        "parser.add_argument('--num_runs', type=int, default=0)\n",
        "parser.add_argument('--env_name', type=str, default=\"Safexp-PointGoal1-v0\")\n",
        "parser.add_argument('--exp_name', type=str, default='ddpg-9gamma-rawruntil')\n",
        "args = parser.parse_args()\n",
        "train_ddpg(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Yj8VtAPUUe"
      },
      "source": [
        "`distill_into_sm` distills the trajectories from the neural network into a state machine using Jeevana's method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU12cuTMZd24"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/home/sdwivedi/f16_propel/smpolicysynth/')\n",
        "\n",
        "from synth.main.learn_conds import * \n",
        "from synth.main.learn_modes import * \n",
        "from synth.policy.prob_state_machine import * \n",
        "from synth.policy.state_machine import *\n",
        "from synth.policy_grammars.straight_traj_grammar import *\n",
        "from synth.policy_grammars.traj_opt_wrapper import *\n",
        "\n",
        "\n",
        "def distill_into_sm(h, gym_env):\n",
        "    gym_env.reset()\n",
        "    nm_unroll = 8\n",
        "    nm_sm = 2\n",
        "    timesteps = 7\n",
        "    cond_depth = 2\n",
        "\n",
        "    pg  = StraightTrajGrammar(gym_env, nm_unroll, timesteps)\n",
        "    opt = TrapOptWrapper(gym_env, pg, 1)\n",
        "\n",
        "    opt.init_full_policy()\n",
        "\n",
        "    trajs = []\n",
        "    trajs.append(opt.get_policy()[0])\n",
        "\n",
        "    curr_state = opt.init_states[0]\n",
        "    for i in range(len(trajs[0].modes)):\n",
        "        action = h.act(torch.as_tensor(curr_state, dtype=torch.float32))\n",
        "        nxt_state, _, _, _ = gym_env.step(action)\n",
        "        curr_state = nxt_state\n",
        "        print(action)\n",
        "        print(trajs[0].modes[i])\n",
        "        for j in range(len(action)):\n",
        "            trajs[0].modes[i][j][0] = action[j]\n",
        "\n",
        "    actions_mean, actions_std, mode_mapping = learn_modes_n_mapping(gym_env, opt.init_states, trajs, [1], None, nm_sm)\n",
        "    conds, conds_std = optimize_conds(gym_env, opt.init_states, trajs,  mode_mapping, nm_sm, [1], cond_depth)\n",
        "\n",
        "    sm = ProbStateMachinePolicy(gym_env, actions_mean, actions_std, conds, conds_std)\n",
        "    return sm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxtlJmSHSoUS"
      },
      "source": [
        "`train_via_drl` method updates neural policy f, taking actions from a linear combination of f and g"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIYfdm9gSkPY"
      },
      "source": [
        "def train_via_drl(f, g, lambda_, gym_env):\n",
        "    o, ep_ret, ep_len = gym_env.reset(), 0, 0\n",
        "    odim = len(gym_env.observation_space.high)\n",
        "    adim = len(gym_env.action_space.high)\n",
        "    replay_buffer = ReplayBuffer(obs_dim=odim, act_dim=adim, size=32)\n",
        "    for t in range(32):\n",
        "        a1 = f.act(torch.as_tensor(o, dtype=torch.float32))\n",
        "        a2 = g.get_action(o) \n",
        "        a1 = np.array(a1)\n",
        "        a2 = np.array(a2)\n",
        "        print(a1)\n",
        "        print(a2)\n",
        "        a = a1\n",
        "        if(len(a2) != 0 ):\n",
        "            a  = a + lambda_*a2\n",
        "        o2, r, d, _ = gym_env.step(a)\n",
        "        ep_ret += r\n",
        "        ep_len += 1\n",
        "        replay_buffer.store(o, a, r, o2, d)\n",
        "    o = o2  \n",
        "    batch = replay_buffer.sample_batch(32)\n",
        "    pi_optimizer = Adam(f.pi.parameters(), lr=0.001)\n",
        "    o = batch['obs']\n",
        "    q_pi = f.q(o, f.pi(o))\n",
        "    loss_pi = -q_pi.mean()\n",
        "    loss_pi.backward()\n",
        "    pi_optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqmNR2cNXynL"
      },
      "source": [
        "Next, we use this model for applying PROPEL with Jeevana's projection operator. We call the method `smpropel`. The algorithm is as follows:\n",
        "\n",
        "1. Learn neural policy f\n",
        "2. h := f  \n",
        "3. Distill h into state machine policy g\n",
        "    DAGGER:\n",
        "    Iteratively generate trajectories from h, then\n",
        "    Update g (Using Jeevana's algorithm)\n",
        "4. Lift g, (linear combination with f)  h := g + \\lambda f\n",
        "5. Neural updates:\n",
        "6. f: = Train_via_DRL(h) [take actions from h, apply gradient updates on f]\n",
        "7. h := g + \\lambda f \n",
        "8. Go to step 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo9rmlzWbEXF"
      },
      "source": [
        "\n",
        "# propel with jeevana's projection operator\n",
        "def smpropel():\n",
        "    # load the saved policy\n",
        "    gym_env = env_fn()\n",
        "    ddpg_model = MLPActorCritic(gym_env.observation_space, gym_env.action_space, hidden_sizes=(256,256), activation=nn.ReLU) \n",
        "    ddpg_model.load_state_dict(torch.load('/home/sdwivedi/f16_propel/ddpg_model.pth'))\n",
        "    num_iterations = 10000\n",
        "    lambda_ = 0.3 \n",
        "    f = ddpg_model\n",
        "    for i in range(num_iterations):\n",
        "        # distill h into state machine policy\n",
        "        g = distill_into_sm(f, gym_env)\n",
        "        train_via_drl(f, g, lambda_, gym_env)\n",
        "        if(num_iterations%10):\n",
        "            g.save('f16_sm') # save the state machine as f16_sm\n",
        "    # h = g + lambda_*f -> This operation happens inside train_via_drl method\n",
        "    return g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8U0yoWiKT1t"
      },
      "source": [
        "Next we test this the state machine obtained from the above algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fhlvIoPKTX9"
      },
      "source": [
        "min_sm = ProbStateMachinePolicy(env_fn(), [], [], [], [])\n",
        "min_sm.read(\"f16_sm\")\n",
        "print(min_sm.evaluate(np.random.choice(10, 13)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}